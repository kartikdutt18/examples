{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@file mnist-cnn-cpp.ipynb\n",
    "\n",
    "An example of using Convolutional Neural Network (CNN) for solving Digit Recognizer problem from Kaggle website.\n",
    "\n",
    "The full description of a problem as well as datasets for training and testing are available here https://www.kaggle.com/c/digit-recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/core.hpp>\n",
    "#include <mlpack/core/data/split_data.hpp>\n",
    "#include <mlpack/methods/ann/layer/layer.hpp>\n",
    "#include <mlpack/methods/ann/init_rules/he_init.hpp>\n",
    "#include <mlpack/methods/ann/ffn.hpp>\n",
    "#include <ensmallen.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * Returns labels bases on predicted probability (or log of probability)\n",
    " * of classes.\n",
    " *\n",
    " * @param predOut matrix contains probabilities (or log of probability) of\n",
    " *     classes. Each row corresponds to a certain class, each column corresponds to a data point.\n",
    " * @return a row vector of data points classes. The classes starts from 1 to the number of rows in input matrix.\n",
    " */\n",
    "arma::Row<size_t> getLabels(const arma::mat& predOut)\n",
    "{\n",
    "  arma::Row<size_t> pred(predOut.n_cols);\n",
    "\n",
    "  // Class of a j-th data point is chosen to be the one with maximum value\n",
    "  // in j-th column plus 1 (since column's elements are numbered from 0).\n",
    "  for (size_t j = 0; j < predOut.n_cols; ++j)\n",
    "  {\n",
    "    pred(j) = arma::as_scalar(arma::find(\n",
    "        arma::max(predOut.col(j)) == predOut.col(j), 1)) + 1;\n",
    "  }\n",
    "\n",
    "  return pred;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * Returns the accuracy (percentage of correct answers).\n",
    " *\n",
    " * @param predLabels predicted labels of data points.\n",
    " * @param realY real labels (they are double because we usually read them from CSV file that contain many other double values).\n",
    " * @return percentage of correct answers.\n",
    " */\n",
    "double accuracy(arma::Row<size_t> predLabels, const arma::mat& realY)\n",
    "{\n",
    "  // Calculating how many predicted classes are coincide with real labels.\n",
    "  size_t success = 0;\n",
    "  for (size_t j = 0; j < realY.n_cols; j++) {\n",
    "    if (predLabels(j) == std::round(realY(j))) {\n",
    "      ++success;\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Calculating percentage of correctly classified data points.\n",
    "  return (double)success / (double)realY.n_cols * 100.0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * Saves prediction into specifically formated CSV file, suitable for\n",
    " * most Kaggle competitions.\n",
    " * @param filename the name of a file.\n",
    " * @param header the header in a CSV file.\n",
    " * @param predLabels predicted labels of data points. Classes of data points\n",
    " * are expected to start from 1. At the same time classes of data points in\n",
    " * the file are going to start from 0 (as Kaggle usually expects)\n",
    " */\n",
    "void save(const std::string filename, std::string header,\n",
    "  const arma::Row<size_t>& predLabels)\n",
    "{\n",
    "  std::ofstream out(filename);\n",
    "  out << header << std::endl;\n",
    "  for (size_t j = 0; j < predLabels.n_cols; ++j)\n",
    "  {\n",
    "    // j + 1 because Kaggle indexes start from 1\n",
    "    // pred - 1 because 1st class is 0, 2nd class is 1 and etc.\n",
    "    out << j + 1 << \",\" << std::round(predLabels(j)) - 1;\n",
    "    // To avoid an empty line in the end of the file.\n",
    "    if (j < predLabels.n_cols - 1)\n",
    "    {\n",
    "      out << std::endl;\n",
    "    }\n",
    "  }\n",
    "  out.close();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Dataset is randomly split into validation\n",
    "// and training parts with following ratio.\n",
    "constexpr double RATIO = 0.1;\n",
    "\n",
    "// Number of iteration per cycle.\n",
    "constexpr int ITERATIONS_PER_CYCLE = 10000;\n",
    "\n",
    "// Number of cycles.\n",
    "constexpr int CYCLES = 40;\n",
    "\n",
    "// Step size of the optimizer.\n",
    "constexpr double STEP_SIZE = 1.2e-3;\n",
    "\n",
    "// Number of data points in each iteration of SGD.\n",
    "constexpr int BATCH_SIZE = 32;\n",
    "\n",
    "// Maximum number of with Epochs.\n",
    "constexpr int EPOCHS = 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Labeled dataset that contains data for training is loaded from CSV file. Rows represent features, columns represent data points.\n",
    "arma::mat tempDataset;\n",
    "\n",
    "// The original file can be downloaded from https://www.kaggle.com/c/digit-recognizer/data\n",
    "data::Load(\"mnist-train.csv\", tempDataset, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The original Kaggle dataset CSV file has headings for each column, so it's necessary to get rid of the first row. In Armadillo representation, this corresponds to the first column of our data matrix.\n",
    "arma::mat dataset = tempDataset.submat(0, 1, tempDataset.n_rows - 1, tempDataset.n_cols - 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Split the dataset into training and validation sets.\n",
    "arma::mat train, valid;\n",
    "data::Split(dataset, train, valid, RATIO);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The train and valid datasets contain both - the features as well as the\n",
    "// class labels. Split these into separate mats.\n",
    "const arma::mat trainX = train.submat(1, 0, train.n_rows - 1, train.n_cols - 1);\n",
    "const arma::mat validX = valid.submat(1, 0, valid.n_rows - 1, valid.n_cols - 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// According to NegativeLogLikelihood output layer of NN, labels should\n",
    "// specify class of a data point and be in the interval from 1 to\n",
    "// number of classes (in this case from 1 to 10).\n",
    "\n",
    "// Create labels for training and validatiion datasets.\n",
    "const arma::mat trainY = train.row(0) + 1;\n",
    "const arma::mat validY = valid.row(0) + 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Specify the NN model. NegativeLogLikelihood is the output layer that\n",
    "// is used for classification problem. RandomInitialization means that\n",
    "// initial weights are generated randomly in the interval from -1 to 1.\n",
    "FFN<NegativeLogLikelihood<>, HeInitialization> model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Specify the model architecture.\n",
    "// In this example, the CNN architecture is chosen similar to LeNet-5.\n",
    "// The architecture follows a Conv-ReLU-Pool-Conv-ReLU-Pool-Dense schema. We\n",
    "// have used leaky ReLU activation instead of vanilla ReLU. Standard\n",
    "// max-pooling has been used for pooling. The first convolution uses 6 filters\n",
    "// of size 5x5 (and a stride of 1). The second convolution uses 16 filters of\n",
    "// size 5x5 (stride = 1). The final dense layer is connected to a softmax to\n",
    "// ensure that we get a valid probability distribution over the output classes\n",
    "\n",
    "// Layers schema.\n",
    "// 28x28x1 --- conv (6 filters of size 5x5. stride = 1) ---> 24x24x6\n",
    "// 24x24x6 --------------- Leaky ReLU ---------------------> 24x24x6\n",
    "// 24x24x6 --- max pooling (over 2x2 fields. stride = 2) --> 12x12x6\n",
    "// 12x12x6 --- conv (16 filters of size 5x5. stride = 1) --> 8x8x16\n",
    "// 8x8x16  --------------- Leaky ReLU ---------------------> 8x8x16\n",
    "// 8x8x16  --- max pooling (over 2x2 fields. stride = 2) --> 4x4x16\n",
    "// 4x4x16  ------------------- Dense ----------------------> 10\n",
    "\n",
    "// Add the first convolution layer.\n",
    "model.Add<Convolution<> >(\n",
    "  1,  // Number of input activation maps.\n",
    "  6,  // Number of output activation maps.\n",
    "  5,  // Filter width.\n",
    "  5,  // Filter height.\n",
    "  1,  // Stride along width.\n",
    "  1,  // Stride along height.\n",
    "  0,  // Padding width.\n",
    "  0,  // Padding height.\n",
    "  28, // Input width.\n",
    "  28  // Input height.\n",
    "  );\n",
    "\n",
    "// Add first ReLU.\n",
    "model.Add<LeakyReLU<> >();\n",
    "\n",
    "// Add first pooling layer. Pools over 2x2 fields in the input.\n",
    "model.Add<MaxPooling<> >(\n",
    "  2,  // Width of field.\n",
    "  2,  // Height of field.\n",
    "  2,  // Stride along width.\n",
    "  2,  // Stride along height.\n",
    "  true\n",
    "  );\n",
    "\n",
    "// Add the second convolution layer.\n",
    "model.Add<Convolution<> >(\n",
    "  6,  // Number of input activation maps.\n",
    "  16, // Number of output activation maps.\n",
    "  5,  // Filter width.\n",
    "  5,  // Filter height.\n",
    "  1,  // Stride along width.\n",
    "  1,  // Stride along height.\n",
    "  0,  // Padding width.\n",
    "  0,  // Padding height.\n",
    "  12, // Input width.\n",
    "  12  // Input height.\n",
    "  );\n",
    "\n",
    "// Add the second ReLU.\n",
    "model.Add<LeakyReLU<> >();\n",
    "\n",
    "// Add the second pooling layer.\n",
    "model.Add<MaxPooling<> >(2, 2, 2, 2, true);\n",
    "\n",
    "// Add the final dense layer.\n",
    "model.Add<Linear<> >(16*4*4, 10);\n",
    "model.Add<LogSoftMax<> >();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "constexpr int MAX_ITERATIONS = 0;\n",
    "ens::SGD<ens::AdamUpdate> optimizer(\n",
    "       // Step size of the optimizer.\n",
    "       STEP_SIZE,\n",
    "       // Batch size. Number of data points that are used in each iteration.\n",
    "       BATCH_SIZE,\n",
    "       // Max number of iterations\n",
    "       EPOCHS * trainY.n_cols,\n",
    "       // Tolerance, used as a stopping condition. This small number\n",
    "       // means we never stop by this condition and continue to optimize\n",
    "       // up to reaching maximum of iterations.\n",
    "       1e-8,\n",
    "       // Shuffle. If optimizer should take random data points from the dataset\n",
    "       // at each iteration.\n",
    "       true,\n",
    "       // Adam update policy.\n",
    "       ens::AdamUpdate(1e-8, 0.9, 0.999));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n",
      "0.241024\n",
      "1182/1182 [==================================================] 100% - 3s 104ms/step - loss: 0.241024\n",
      "Epoch 2/160\n",
      "0.0867259\n",
      "1182/1182 [==================================================] 100% - 3s 104ms/step - loss: 0.0867259\n",
      "Epoch 3/160\n",
      "0.0733466\n",
      "1182/1182 [==================================================] 100% - 3s 104ms/step - loss: 0.0733466\n",
      "Epoch 4/160\n",
      "0.0649816\n",
      "1182/1182 [==================================================] 100% - 3s 104ms/step - loss: 0.0649816\n",
      "Epoch 5/160\n",
      "0.0616594\n",
      "1182/1182 [==================================================] 100% - 3s 104ms/step - loss: 0.0616594\n"
     ]
    }
   ],
   "source": [
    " model.Train(trainX,\n",
    "             trainY,\n",
    "             optimizer,\n",
    "             ens::PrintLoss(),\n",
    "             ens::ProgressBar(50),\n",
    "             // Stop the training using Early Stop at min loss.\n",
    "             ens::EarlyStopAtMinLoss());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  valid = 96.7381\n"
     ]
    }
   ],
   "source": [
    "arma::mat predOut;\n",
    "// Getting predictions on training data points.\n",
    "model.Predict(trainX, predOut);\n",
    "// Calculating accuracy on training data points.\n",
    "arma::Row<size_t> predLabels = getLabels(predOut);\n",
    "// Getting predictions on validating data points.\n",
    "model.Predict(validX, predOut);\n",
    "// Calculating accuracy on validating data points.\n",
    "predLabels = getLabels(predOut);\n",
    "double validAccuracy = accuracy(predLabels, validY);\n",
    "\n",
    "std::cout << \"Accuracy:  valid = \"<< validAccuracy << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting ...\n"
     ]
    }
   ],
   "source": [
    "std::cout << \"Predicting ...\" << std::endl;\n",
    "// Load test dataset. The original file could be download from https://www.kaggle.com/c/digit-recognizer/data.\n",
    "data::Load(\"mnist-test.csv\", tempDataset, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predicted labels to results.csv.\n"
     ]
    }
   ],
   "source": [
    "// As before, it's necessary to get rid of column headings.\n",
    "arma::mat testX = tempDataset.submat(0, 1, tempDataset.n_rows - 1, tempDataset.n_cols - 1);\n",
    "// Matrix to store the predictions on test dataset.\n",
    "arma::mat testPredOut;\n",
    "\n",
    "// Get predictions on test data points.\n",
    "model.Predict(testX, testPredOut);\n",
    "// Generate labels for the test dataset.\n",
    "arma::Row<size_t> testPred = getLabels(testPredOut);\n",
    "std::cout << \"Saving predicted labels to results.csv.\"<< std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results were saved to kaggel-results.csv. This file can be uploaded to https://www.kaggle.com/c/digit-recognizer/submissions.\n"
     ]
    }
   ],
   "source": [
    "// Saving results into Kaggle compatibe CSV file.\n",
    "save(\"kaggel-results.csv\", \"ImageId,Label\", testPred);\n",
    "std::cout << \"Results were saved to kaggel-results.csv. This file can be uploaded to \"\n",
    "    << \"https://www.kaggle.com/c/digit-recognizer/submissions.\" << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++11",
   "language": "C++11",
   "name": "xcpp11"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
